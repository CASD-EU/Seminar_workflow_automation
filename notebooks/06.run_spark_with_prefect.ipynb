{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Run spark with prefect\n",
    "\n",
    "In CASD, we recommend our clients to use `Prefect orchestrates tasks and flows(e.g. scheduling, retries, dependencies, observability)`, while `PySpark executes distributed computations(e.g. data processing, transformation, etc.)`.\n",
    "\n",
    "> That's why we do not recommend our clients use Prefect parallelism to do data processing."
   ],
   "id": "3ad32e6e5c1246d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Integrate spark into prefect task\n",
    "\n",
    "There are two options:\n",
    "- In process SparkSession\n",
    "- Use command line spark-submit (Not recommended for Windows server)"
   ],
   "id": "bc595d84d8b0ede0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from platform import python_branch\n",
    "\n",
    "### 1.1 A simple example\n",
    "\n",
    "In the below example, we define a workflow which has two tasks:\n",
    "- task1: read a text file, count numbers of unique words, then write result in a csv file\n",
    "- task2: read the output csv file of task 1, filter the results with a given list\n",
    "\n",
    "```python\n",
    "import logging\n",
    "\n",
    "from prefect import flow, task, get_run_logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# the partial file path works, because the prefect worker and spark runs on mode local\n",
    "# if the prefect worker and spark worker runs on remote server. The path won't work.\n",
    "data_dir = \"../../data\"\n",
    "# data_dir = \"C:/Users/PLIU/Documents/git/Seminar_workflow_automation/data\"\n",
    "\n",
    "# you need to change the username value. So the spark.local.dir file path is dedicated to your environment to avoid file access conflict.\n",
    "user_name = \"pengfei\"\n",
    "\n",
    "\n",
    "@task(name=\"task_1\",\n",
    "      description=\"task 1 read a text file, count numbers of unique words, then write result in a csv file\")\n",
    "def wordcount_task(source_file: str, out_file: str):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"prefect_wordcount\")\n",
    "        .master(\"local[4]\")  # Limit CPU usage\n",
    "        .config(\"spark.local.dir\", f\"{data_dir}/spark_temp/{user_name}\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    df = spark.read.text(source_file)\n",
    "    counts = df.rdd.flatMap(lambda x: x[0].split()) \\\n",
    "        .map(lambda w: (w, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    counts.toDF([\"word\", \"count\"]).write.mode(\"overwrite\").csv(out_file)\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "@task(name=\"task_2\", description=\"task 2 reads the output csv file of task 1, filter the results with a given list\")\n",
    "def filter_task(source_file: str, out_file: str, target_words: list[str]):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"prefect_word_filter\")\n",
    "        .master(\"local[4]\")  # Limit CPU usage\n",
    "        .config(\"spark.local.dir\", f\"{data_dir}/spark_temp/{user_name}\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    schema = StructType([\n",
    "        StructField(\"word\", StringType(), True),\n",
    "        StructField(\"count\", IntegerType(), True)])\n",
    "\n",
    "    df = spark.read.csv(source_file, header=False, schema=schema)\n",
    "    result = df.filter(col(\"word\").isin(target_words))\n",
    "    result.write.mode(\"overwrite\").csv(out_file)\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "@flow(name=\"spark_wordcount_flow\",\n",
    "      description=\"This workflow read plain text file and count words, we handle the error with task state\",\n",
    "      version=\"1.0.0\")\n",
    "def main_flow(target_words: list[str]):\n",
    "    # set up logger\n",
    "    logger = get_run_logger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # run task 1\n",
    "    src_file1 = f\"{data_dir}/source/word_raw.txt\"\n",
    "    out_file1 = f\"{data_dir}/out/wc_out\"\n",
    "    t1_state = wordcount_task(src_file1, out_file1, return_state=True)\n",
    "    # check task 1 state, before start task2\n",
    "    if t1_state.is_completed():\n",
    "        # run task 2\n",
    "        out_file2 = f\"{data_dir}/out/flow_out\"\n",
    "        filter_task(out_file1, out_file2, target_words)\n",
    "    else:\n",
    "        logger.error(\"The task 1 does not complete with success, no need to run the task 2.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_words = [\"data\", \"file\"]\n",
    "    main_flow(target_words)\n",
    "\n",
    "```\n",
    "\n",
    "Pay attention:\n",
    "- the file path configuration\n",
    "- why each task has a spark session definition?\n",
    "- how the error handling is done?"
   ],
   "id": "4230f80c867dd24b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Resource concurrency\n",
    "\n",
    "As prefect allows users to run workflow in parallel, if we don't set up `concurrency limit`. we may face the server overload problem.\n",
    "\n",
    "For example, if the server has `8VCore, and 32GB memory`, we have three workflows each takes 4Vcores and 16GB memory(i.e. a typical spark job). We won't have enough resources to run the three workflows correctly.\n",
    "But the server will continuously try to finish the workflow with `fairness`. Which means give 10 secs for workflow 1, then 10 secs for workflow2, etc. Because it does not have enough for every workflow at the same time. In this scenario, you will experience lag everywhere, even the Windows file explorer.\n",
    "\n",
    "To avoid this condition, CASD propose the below set-up(Here, we suppose the server has 8VCore, and 32GB memory):\n",
    "- Global concurrency limit = 2 (it means only two workflows can run simultaneously)\n",
    "- `work-pool=1` and attached `worker concurrency limit=1` for each user. (one user can run a workflow at the same time)\n",
    "- spark session setup must have two options: `local[4]`(uses 4 core), .config(\"spark.driver.memory\", \"8g\")\n",
    "\n",
    "> do not use `local[*]` when create spark session. This will use all cpu of the server.\n",
    "\n",
    "The below figure shows an example. Four users launch a workflow at the same time, 16 cores are required, with our setup, only two workflows can run at the same, the other two workflows will be waiting in the work-pool queue. So we will not have the overflow situation\n",
    "\n",
    "```text\n",
    "                ┌────────────────────────────────────────────┐\n",
    "                │              Prefect Server                │\n",
    "                └────────────────────────────────────────────┘\n",
    "                                  │\n",
    "                        (global concurrency limit, limit=2)\n",
    "                                  │\n",
    "      ┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "      │ User A Worker    │ User B Worker    │ User C Worker    │ User D Worker    │\n",
    "      │ --limit 1        │ --limit 1        │ --limit 1        │ --limit 1        │\n",
    "      └──────────────────┴──────────────────┴──────────────────┴──────────────────┘\n",
    "        Spark  │  local[4]  Spark │  local[4]  Spark │ local[4]  Spark │ local[4]\n",
    "               ▼                  ▼                  ▼                 ▼\n",
    "        4 cores required     4 cores required   4 cores required   4 cores required\n",
    "\n",
    "```\n",
    "\n"
   ],
   "id": "e1f6dba828161a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Use spark submit in prefect (Advance)\n",
    "\n",
    "You can try to use the below command to run a spark job via spark-submit\n",
    "\n",
    "```powershell\n",
    "spark-submit --master \"local[4]\" --conf \"spark.driver.memory=4g\" .\\projects\\05_run_spark_with_prefect\\spark_jobs\\word_count.py \".\\data\\source\\word_raw.txt\" \".\\data\\out\\spark_submit_out\"\n",
    "```\n",
    "\n"
   ],
   "id": "868979d95cacc59f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
