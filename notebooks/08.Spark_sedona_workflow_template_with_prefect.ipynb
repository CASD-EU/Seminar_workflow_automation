{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# spark sedona prefect workflow template\n",
    "\n",
    "\n",
    "## 1. Why we cannot Use a Global SparkSession in Prefect\n",
    "\n",
    "### 1.1 Process boundaries\n",
    "\n",
    "In our case, we use a `process work-pool`. Each task runs in a separate subprocess (multiprocessing)\n",
    "\n",
    "Global variables in workflow definition(main process) do not cross process boundaries, which means the spark session is not visible inside task worker processes(subprocess).\n",
    "\n",
    "### 1.2 SparkSession cannot be pickled\n",
    "\n",
    "You may ask if the main process sent the spark session to subprocess, there will be no more problems.\n",
    "\n",
    "Prefect serializes parameters and state using Pydantic + cloudpickle.\n",
    "\n",
    "A SparkSession contains:\n",
    "- Java gateway references\n",
    "- Socket connections\n",
    "- JVM pointers\n",
    "- Py4J handles\n",
    "\n",
    "These cannot be serialized. A task receiving a SparkSession argument will raise: `A task receiving a SparkSession argument will raise:`\n",
    "\n",
    "### 1.3 Spark is NOT thread-safe.\n",
    "\n",
    "Two tasks using one Spark session at the same time â†’ race conditions:\n",
    "- JVM deadlocks\n",
    "- Out-of-memory crashes\n",
    "- Inconsistent job runs\n",
    "- Spark UI showing mixed jobs from different tasks\n",
    "\n",
    "### 1.4 Global SparkSession breaks worker restart & resiliency\n",
    "\n",
    "If a worker crashes:\n",
    "\n",
    "- Prefect restarts the process\n",
    "- Any global SparkSession state is gone\n",
    "- Tasks that depend on that global session cannot recover\n",
    "\n",
    "Prefect's engine assumes tasks are stateless and reproducible. Global spark session state violates this model.\n",
    "\n",
    "## Conclusion: we must create a spark session in each task\n",
    "\n",
    "## Best practices\n",
    "\n",
    "To avoid duplicate the spark session creation code, we need a factory function which can build spark session automatically\n",
    "\n"
   ],
   "id": "914d6405e14e7e11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:18:06.154921Z",
     "start_time": "2025-12-04T15:18:05.909484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "def build_spark_temp_dir() -> str:\n",
    "    \"\"\"\n",
    "    This function creates a temporary directory for Spark temporary files. Each Spark session uses a unique temp folder will avoid\n",
    "    file-lock conflicts on Windows. Folder can be safely deleted after spark.stop().\n",
    "    An example output C:/Users/alice/AppData/Local/spark_temp/20250329_124501_839201\n",
    "    :return: the temporary directory path in string\n",
    "    \"\"\"\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    base = Path(tempfile.gettempdir())\n",
    "    spark_temp_dir = base / \"spark_temp\" / f\"spark_{ts}\"\n",
    "    spark_temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return spark_temp_dir.as_posix()\n"
   ],
   "id": "eaad66ce8525ebd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:18:07.531048Z",
     "start_time": "2025-12-04T15:18:07.525390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_spark_session(app_name: str, extra_jar_folder_path: str | None = None, driver_memory: str = \"4g\", local_dir_path: str | None = None):\n",
    "    \"\"\"\n",
    "    This function builds a SparkSession with standardized configuration and extra dependencies.\n",
    "\n",
    "    :param app_name:\n",
    "    :param extra_jar_folder_path:\n",
    "    :param driver_memory:\n",
    "    :param local_dir_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create a standard spark session builder\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[4]\")\n",
    "        .config(\"spark.driver.memory\", driver_memory)\n",
    "        # enable AQE\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        # give a partition size advice.\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n",
    "        # set AQE partition range\n",
    "        .config(\"spark.sql.adaptive.maxNumPostShufflePartitions\", \"100\")\n",
    "        .config(\"spark.sql.adaptive.minNumPostShufflePartitions\", \"1\")\n",
    "        # increase worker timeout\n",
    "        .config(\"spark.network.timeout\", \"800s\")\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"90s\")\n",
    "        .config(\"spark.sql.sources.commitProtocolClass\",\n",
    "                \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "        # JVM memory allocation\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "        # Shuffle & partition tuning\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "        .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "        # Unified memory management\n",
    "        .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "        # Spill to disk early instead of crashing\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\n",
    "        .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "        .config(\"spark.shuffle.compress\", \"true\")\n",
    "        # optimize jvm GC\n",
    "        .config(\"spark.driver.extraJavaOptions\",\n",
    "                \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "        # Use Kryo serializer\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        # Optional: buffer size for serialization\n",
    "        .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    )\n",
    "\n",
    "    # Ensure local_dir exists\n",
    "    # if local dir path are not provided, generate a standard temp folder\n",
    "    if local_dir_path is None:\n",
    "        local_dir_path = build_spark_temp_dir()\n",
    "    else:\n",
    "        local_dir = Path(local_dir_path)\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    builder = builder.config(\"spark.local.dir\", local_dir_path)\n",
    "\n",
    "    # Load extra JARs only when present\n",
    "    if extra_jar_folder_path:\n",
    "        jar_dir = Path(extra_jar_folder_path)\n",
    "        jar_files = [\n",
    "            str(p) for p in jar_dir.iterdir()\n",
    "            if p.is_file() and p.suffix == \".jar\"\n",
    "        ]\n",
    "        if jar_files:\n",
    "            builder = builder.config(\"spark.jars\", \",\".join(jar_files))\n",
    "\n",
    "    return builder.getOrCreate(), local_dir_path"
   ],
   "id": "5dfaed121d8780bc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:22:16.081031Z",
     "start_time": "2025-12-04T15:22:16.075243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def safe_delete(target_path: str, retries: int = 5, delay: float = 0.5):\n",
    "    \"\"\"\n",
    "    This function cleans the spark temp dir after the spark session is closed\n",
    "    :param target_path:\n",
    "    :param retries:\n",
    "    :param delay:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    target = Path(target_path)\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if not target.exists():\n",
    "            return True     # Already deleted\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree(target)\n",
    "        except PermissionError:\n",
    "            time.sleep(delay * attempt)   # exponential backoff\n",
    "\n",
    "        # Double-check if deletion actually succeeded\n",
    "        if not target.exists():\n",
    "            return True\n",
    "\n",
    "    return False  # Explicit failure after retries"
   ],
   "id": "e8b6454b1ec31065",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:18:13.505713Z",
     "start_time": "2025-12-04T15:18:13.501978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# here we choose sedona 1.8.0 for spark 3.5.* build with scala 2.12\n",
    "project_root_dir = Path.cwd().parent\n",
    "sedona_version = \"sedona-35-212-180\"\n",
    "jar_folder_path = f\"{project_root_dir}/jars/{sedona_version}\""
   ],
   "id": "1611875fa564b551",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:18:43.514316Z",
     "start_time": "2025-12-04T15:18:15.297239Z"
    }
   },
   "cell_type": "code",
   "source": "spark, spark_temp_dir_path = build_spark_session(\"my-sedona-demo\", jar_folder_path, driver_memory=\"6g\")",
   "id": "891a6caa990079c6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:19:08.731738Z",
     "start_time": "2025-12-04T15:18:48.945630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# create a sedona context\n",
    "sedona = SedonaContext.create(spark)"
   ],
   "id": "71017c54f2beb924",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:19:09.425625Z",
     "start_time": "2025-12-04T15:19:08.771708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../data\"\n",
    "src_file = f\"{data_dir}/source/word_raw.txt\"\n",
    "out_file = f\"{data_dir}/out/wc_out\"\n",
    "\n",
    "df = spark.read.text(src_file)\n",
    "\n",
    "df.show()"
   ],
   "id": "ba87bab8ae6f5b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is a test da...|\n",
      "|   data file updated|\n",
      "|data is the new b...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:19:10.324239Z",
     "start_time": "2025-12-04T15:19:09.498999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "words = df.select(explode(split(col(df.columns[0]), \"\\\\s+\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    "counts.show()"
   ],
   "id": "9a3ff4d3a5bddfed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|  gold.|    1|\n",
      "|    new|    1|\n",
      "|    for|    1|\n",
      "|     is|    2|\n",
      "|updated|    1|\n",
      "|   data|    3|\n",
      "| count.|    1|\n",
      "|   file|    2|\n",
      "|    the|    1|\n",
      "|   word|    1|\n",
      "|  black|    1|\n",
      "|   This|    1|\n",
      "|      a|    1|\n",
      "|   test|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:19:11.044553Z",
     "start_time": "2025-12-04T15:19:10.436550Z"
    }
   },
   "cell_type": "code",
   "source": "counts.write.mode(\"overwrite\").csv(out_file)",
   "id": "9a769c1cc837a5e2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:19:11.358644Z",
     "start_time": "2025-12-04T15:19:11.053198Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "34397f3db949bce9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:23:42.375544Z",
     "start_time": "2025-12-04T15:23:27.344637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "safe_delete(spark_temp_dir_path)\n",
    "safe_delete(\"C:/Users/pliu/AppData/Local/Temp/spark_temp/spark_20251204_161815_299461\")"
   ],
   "id": "cd5fc1f30d5ca977",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:23:04.661903Z",
     "start_time": "2025-12-04T15:23:04.658059Z"
    }
   },
   "cell_type": "code",
   "source": "print(spark_temp_dir_path)",
   "id": "b00976e4323a2826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/pliu/AppData/Local/Temp/spark_temp/spark_20251204_161815_299461\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9df73b192a11b313"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
